{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ef63c3",
   "metadata": {},
   "source": [
    "# Query and Accessing Data in a Gen3 Data Commons\n",
    "---\n",
    "by Chris Meyer, PhD\n",
    "\n",
    "Manager of Data and User Services at the Center for Translational Data Science at University of Chicago\n",
    "\n",
    "June 2022\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7f87c",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "---\n",
    "* This notebook is intended to demonstrate a variety of ways to access file objects and structured data (aka \"metadata\") in a Gen3 data commons.\n",
    "* File objects are accessed via their \"data GUID\" aka \"object_id\", which is a unique identifier that is associated with a storage_url in the file index (https://data.midrc.org/index/index). Users must be authorized to access a file in order to download it via the object_id. \n",
    "* Structured data in a Gen3 Data Commons is imported into Postgres via the \"sheepdog\" service and must conform to the data model. The data model is a relational model that consists of tables or \"nodes\" that are related to one another via foreign keys so that the model can be thought of as a graph of nodes that are linked to each other. Each node in the model contains certain properties (keys) that store data of a particular type (values).\n",
    "* The \"sheepdog\" service can export tables of data from a particular node of a data project. This is the simplest way to access \"all\" the data in a Gen3 data commons.\n",
    "* Queries can be constructed to target specific types of data in Postgres and are handled by the \"peregrine\" graphQL service.\n",
    "* Structured data can also be transformed via an \"ETL\" (extract, transform, load) process that takes the complex relationships between nodes and \"flattens\" the data into a single table, which is stored in an ElasticSearch (ES) database that can be queried using the \"guppy\" graphQL service. These ES tables are what the data exploration app of the Gen3 data-portal is based on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a7935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The packages below may be necessary for users to install according to the imports necessary in the subsequent cells.\n",
    "\n",
    "#!pip install --upgrade pandas\n",
    "#!pip install --upgrade --ignore-installed PyYAML\n",
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade gen3\n",
    "#!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Packages and scripts\n",
    "\n",
    "import pandas as pd\n",
    "import sys, os\n",
    "import gen3\n",
    "import pydicom\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "from gen3.submission import Gen3Submission\n",
    "from gen3.auth import Gen3Auth\n",
    "from gen3.index import Gen3Index\n",
    "from gen3.query import Gen3Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e9922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some custom Python scripts from personal GitHub repo\n",
    "# change these directory paths to reflect your local working directory\n",
    "\n",
    "home_dir = \"/Users/christopher\" \n",
    "demo_dir = \"{}/Documents/Notes/MIDRC/tutorials\".format(home_dir)\n",
    "\n",
    "os.chdir(demo_dir)\n",
    "\n",
    "os.system(\"wget https://raw.githubusercontent.com/cgmeyer/gen3sdk-python/master/expansion/expansion.py -O {}/expansion.py\".format(demo_dir))\n",
    "%run expansion.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate instances of the Gen3 SDK Classes using credentials file for authentication\n",
    "# Change the directory path in \"cred\" to reflect the location of your credentials file.\n",
    "\n",
    "api = \"https://data.midrc.org\"\n",
    "cred = \"{}/Downloads/midrc-credentials.json\".format(home_dir)\n",
    "auth = Gen3Auth(api, refresh_file=cred) # authentication class\n",
    "sub = Gen3Submission(api, auth) # submission class\n",
    "query = Gen3Query(auth) # query class\n",
    "exp = Gen3Expansion(api,auth,sub) # class with some custom scripts\n",
    "exp.get_project_ids()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fc475d",
   "metadata": {},
   "source": [
    "## Accessing structured data in Postgres using sheepdog exports\n",
    "---\n",
    "* Probably the most straight-forward way to access structured data in a Gen3 Data Commons is to simply export the table of data using the sheepdog service (https://petstore.swagger.io/?url=https://raw.githubusercontent.com/uc-cdis/sheepdog/master/openapi/swagger.yml#/export/post__program___project__export).\n",
    "* The Gen3SDK has a function `Gen3Submission.export_node()` for exporting entire tables of data from Postgres: https://github.com/uc-cdis/gen3sdk-python/blob/8196cf4b76a65d0b9b31c8637a18dfac2a911b56/gen3/submission.py#L361\n",
    "    * This function will export all records in a particular node of a specified project, and one can then use standard Python / R (etc.) tools to do the filtering and cohort building.\n",
    "* Note: This export function is also accesible in the data-portal by navigating to a data project's URL, e.g., https://data.midrc.org/Open-A1, clicking a node in the graph, and then clicking the \"Download All\" button.\n",
    "    * For example: https://data.midrc.org/Open-A1/search?node_type=measurement\n",
    "    * Or, you can enter this URL in your browser, for example: https://data.midrc.org/api/v0/submission//Open/A1/export?node_label=measurement&format=tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of exporting a table of data using the `Gen3Submission.export_node()` function\n",
    "cases = sub.export_node(program='Open',project='A1',node_type='case',fileformat='tsv')\n",
    "df = pd.read_csv(StringIO(cases), sep='\\t', header=0)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff227add",
   "metadata": {},
   "outputs": [],
   "source": [
    "## One can then use standard tools in any programming language to do cohort building. \n",
    "## Here I'm using the \"pandas\" Python package to select a cohort based on demographic information stored in the case node.\n",
    "cohort = list(df.loc[(df['sex']=='Female') & (df['race']==\"Black or African American\") & (df['age_at_index']>79)]['submitter_id'])\n",
    "display(len(cohort))\n",
    "cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e781d",
   "metadata": {},
   "source": [
    "### Using a Python wrapper to get all the data in a particular node\n",
    "---\n",
    "* I've written a wrapper script called `Gen3Expansion.get_node_tsvs()` that uses the `Gen3Submission.export_node()` function to export the same node across all projects you have access to in the data commons and then merges the results into a single master table for that node:\n",
    "https://github.com/cgmeyer/gen3sdk-python/blob/5fd6b868374f622221c0c0173a0d9489b190facd/expansion/expansion.py#L219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0977e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases = exp.get_node_tsvs(node='case')\n",
    "display(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150f734",
   "metadata": {},
   "source": [
    "### Using a Python wrapper to get all the data in a particular project\n",
    "---\n",
    "* Similar to the above example, I've written a wrapper script called `Gen3Expansion.get_project_tsvs()` that uses the `Gen3Submission.export_node()` function to export every node in every project (or a particular project) in the data commons.\n",
    "https://github.com/cgmeyer/gen3sdk-python/blob/5fd6b868374f622221c0c0173a0d9489b190facd/expansion/expansion.py#L298\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71757809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This example gets all the data in every node of the data model in the project Open-A1\n",
    "## If \"projects\" is not specific, all data across all projects you have access to will be downloaded.\n",
    "exp.get_project_tsvs(projects='Open-A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l project_tsvs/Open-A1_tsvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e40ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can then read in the TSVs of data exported from a node to do cohort building / research\n",
    "tsv_dir = 'project_tsvs/Open-A1_tsvs'\n",
    "ct = pd.read_csv(\"{}/Open-A1_ct_series_file.tsv\".format(tsv_dir),sep='\\t',dtype=str)\n",
    "display(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can use Python to get the CT series files for the cohort of cases we built earlier\n",
    "cohort_ct = ct.loc[ct['case_ids'].isin(cohort)]\n",
    "cohort_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20261819",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can access the cohort's CT series files by using the 'object_id' field:\n",
    "object_ids = list(cohort_ct['object_id'])\n",
    "object_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8e81d",
   "metadata": {},
   "source": [
    "## Queries to Postgres using Peregrine graphQL query service\n",
    "---\n",
    "* Peregrine GitHub Docs: https://github.com/uc-cdis/peregrine\n",
    "* Peregrine swagger docs: https://petstore.swagger.io/?url=https://raw.githubusercontent.com/uc-cdis/peregrine/master/openapis/swagger.yaml\n",
    "\n",
    "---\n",
    "* Most structured data (aka \"metadata\") submitted to a Gen3 system is stored in Postgres tables using the \"sheepdog\" service. This data must conform to the data commons' data model (https://data.midrc.org/dd), and is queryable via the \"peregrine\" service, which converts graphQL queries to SQL queries and returns the data requested. The Postgres tables are considered the \"source-of-truth\" for data in a Gen3 system (vs. the derived data in ElasticSearch, covered below).\n",
    "\n",
    "* On the data commons' website, peregrine queries can be sent to the API using the \"graphiQL\" query builder: https://data.midrc.org/query (click on \"Switch to Graph Model\"; if button says \"Switch to Flat Model\" you're in the correct spot).\n",
    "\n",
    "* Alternatively, you can send queries to the peregrine API using the Gen3SDK `Gen3Submission.query()` function, which uses the Python `requests` package to send queries as API requests: https://github.com/uc-cdis/gen3sdk-python/blob/31751633ba621b35f39eda7295f131245fb92728/gen3/submission.py#L399\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a6d7e",
   "metadata": {},
   "source": [
    "### Example graph model query \\#1\n",
    "* This query is running across all records in the `case` node and returns data from any dataset in the data commons you are authorized to access.  Remember, the properties in the `case` node are essentially table headers for variables whose values are of a specific data type (string, enumeration, integer, number, boolean, array, etc.).\n",
    "* The argument `covid19_positive: \"Yes\"` returns only case records that have the value \"Yes\" for the property `covid19_positive`, which indicates whether a case in MIDRC has ever had a positive COVID-19 test result.\n",
    "* The `first` argument defines how many `case` records we want returned. Using the argument `first: 0`, all the records we have access to will be returned. If we leave the \"first\" argument out, only the first 10 records are returned by default. Setting `first: 2000` will return the first 2000 records in the table, etc.\n",
    "* If your query is timing out, you will need to paginate the query (covered in next section) using a combination of \"first\" and \"offset\" arguments. This is only necessary if the tables being queries are very large, or the query traverses many nodes in the graph model.\n",
    "* Properties we want returned from the API are enclosed in brackets. The possibilities and exact syntax are constrained by the data model (data.midrc.org/dd). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7994cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the query\n",
    "\n",
    "## Here we're asking for the `project_id`, `submitter_id`, and some demographic data for every `case` record.\n",
    "## We're also asking for the `study_uid` for every `imaging_study` record belonging to those cases, and for all `dx_series_file` records for those `imaging_studies`.\n",
    "## Finally, we're asking for the `file_name` and `object_id` of any Digital X-ray files (node `dx_series_file`, backref: `dx_series_files`) they may have.\n",
    "\n",
    "## Note: \"submitter_id\" is a required property on every node, which is the human-readable (string), unique identifier for a record in a data table / node. So, the \"submitter_id\" of a record in the case node is the de-identified patient's \"ID\".\n",
    "\n",
    "query_txt = \"\"\"\n",
    "{\n",
    "  case(first: 0, covid19_positive: \"Yes\") {\n",
    "    project_id\n",
    "    submitter_id\n",
    "    ethnicity\n",
    "    sex\n",
    "    race\n",
    "    imaging_studies (study_modality: \"DX\") {\n",
    "      study_uid\n",
    "      dx_series_files {\n",
    "        object_id\n",
    "        file_name\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cecf28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Send the query using the Gen3 SDK Gen3Submission.query() function\n",
    "## The response will be in JSON format.\n",
    "\n",
    "response = sub.query(query_txt)\n",
    "if 'data' in response:\n",
    "    data = response['data']['case']\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the \"object_id\" field is the file's data GUID (or globally unique identifier), which can be used to access the file.\n",
    "\n",
    "object_ids = []\n",
    "for case in data:\n",
    "    studies = case['imaging_studies']\n",
    "    for study in studies:\n",
    "        files = study['dx_series_files'] \n",
    "        if len(files)>0:\n",
    "            for file in files:\n",
    "                object_id = file['object_id']\n",
    "                object_ids.append(object_id)\n",
    "object_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b02d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a look at one of the file objects\n",
    "#object_id was originally selected from the list above.  If the object id is not above use an object id from above.\n",
    "\n",
    "object_id = 'dg.MD1R/ea6ad8e7-1dc9-4916-8e75-38abb66c6416'\n",
    "os.system(\"gen3 --auth {} --endpoint data.midrc.org drs-pull object {}\".format(cred,object_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l 10000364-1958844/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The object downloaded above was zipped and must be unzipped to process further\n",
    "#The path and file names may have changed and those changes should be reflected below\n",
    "!unzip 10000364-1958844/2.16.840.1.114274.1818.52236113359126249589212595743121753735/2.16.840.1.114274.1818.54309100269617797736626917868992258958.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydicom import dcmread\n",
    "\n",
    "fpath = \"2.16.840.1.114274.1818.54309100269617797736626917868992258958/2.16.840.1.114274.1818.46312267929568121457864041736105067915.dcm\"\n",
    "ds = dcmread(fpath)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7308d00f",
   "metadata": {},
   "source": [
    "### Counts with peregrine\n",
    "---\n",
    "* Peregrine is able to provide counts of records in nodes. A simple example is to quickly get the count of the numbers of cases and imaging studies in the data commons.\n",
    "* You can also add arguments to the counts to, for example, get the number of cases in a particular project or get the imaging studies of a particular modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"{_case_count}\"\n",
    "print(sub.query(query_txt))\n",
    "query_txt = \"{_imaging_study_count}\"\n",
    "print(sub.query(query_txt))\n",
    "query_txt = '{CT_studies: _imaging_study_count(study_modality:\"CT\")}'\n",
    "print(sub.query(query_txt))\n",
    "query_txt = '{Open_A1_cases: _case_count(project_id:\"Open-A1\")}'\n",
    "print(sub.query(query_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd297c",
   "metadata": {},
   "source": [
    "### Queries of \"datanode\" using peregrine\n",
    "---\n",
    "Another handy trick with peregrine queries is the \"datanode\" query. \"Datanode\" isn't a real node in the data model, but is useful way to query all nodes that store file information. For example, if you have a patient ID, you can get all the files associated with that case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097f8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt = \"\"\"\n",
    "{\n",
    "  datanode(first: 0, case_ids: \"10000364-1163342\") {\n",
    "    object_id\n",
    "    file_name\n",
    "    modality\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "response = sub.query(query_txt)\n",
    "if 'data' in response:\n",
    "    display(response['data']['datanode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba72458",
   "metadata": {},
   "source": [
    "## Queries to ElasticSearch using Guppy graphQL query service\n",
    "---\n",
    "* Guppy Documentation: https://github.com/uc-cdis/guppy/blob/master/doc/queries.md#filters\n",
    "* Guppy Download instructions: https://github.com/uc-cdis/guppy/blob/master/doc/download.md\n",
    "* ETL (Tube) Documentation: https://github.com/uc-cdis/tube#gen3-etl---a-process-from-postgresql-to-es\n",
    "---\n",
    "* The Gen3 platform includes services for running an ETL process (Extract, Transform, Load), which is done by the Gen3 ETL service \"tube\", on the data in Postgres to create flattened tables of the same data in ElasticSearch (ES) for rapid querying performed by the Gen3 query service \"guppy\".\n",
    "* Guppy runs graphql-like queries against the ES database, and can rapidly return derived data like histograms, statistics, aggregations, counts, etc. The tube service uses Spark to create these new tables of data in ES via an ETL mapping, which defines the structure of the new tables and is based on the data model. \n",
    "* Since the structure of the data changes via the ETL process, peregrine queries to Postgres will not run using guppy. To explore what is possible to query, use the graphiQL interface / documentation.\n",
    "* The \"Exploration\" app aka \"Data Explorer\" (data.midrc.org/explore), which uses faceted search to filter the flat data tables in ES, runs off of guppy queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## an example guppy query, which hits the ElasticSearch database\n",
    "\n",
    "## define some parameters\n",
    "pid = 'Open-R1'\n",
    "node = 'imaging_study'\n",
    "fields = [\"study_uid\",\n",
    "    \"study_description\",\n",
    "    \"case_ids\",\n",
    "    \"object_id\"]\n",
    "filters = {\"project_id\": pid,\n",
    "    \"covid19_positive\" : \"Yes\",\n",
    "    \"body_part_examined\" : \"CHEST\",\n",
    "    \"study_modality\" : \"DX\"}\n",
    "\n",
    "## send the guppy query with the SDK class Gen3Query\n",
    "## Note the \"first: 100000\", which makes sure we don't just get the default first 10 records\n",
    "response = query.query(\n",
    "                data_type=node,\n",
    "                first=100000,\n",
    "                fields=fields,\n",
    "                filters=filters,\n",
    "                sort_object={\"submitter_id\": \"asc\"},\n",
    ")\n",
    "\n",
    "# display the returned data\n",
    "if 'data' in response:\n",
    "    study_data = response['data'][node]\n",
    "    display(study_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4459fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## another example guppy query, which hits the ElasticSearch database\n",
    "\n",
    "## define some parameters\n",
    "node = 'case'\n",
    "\n",
    "fields = [\"project_id\",\n",
    "    \"submitter_id\",\n",
    "    \"object_id\"]\n",
    "\n",
    "filters = {\"sex\":\"Female\",\n",
    "    \"race\" : \"Asian\",\n",
    "    \"ethnicity\" : \"Hispanic or Latino\"}\n",
    "\n",
    "## send the guppy query with the SDK class Gen3Query\n",
    "## Note the \"first: 100000\", which makes sure we don't just get the default first 10 records\n",
    "response = query.query(\n",
    "                data_type=node,\n",
    "                first=100000,\n",
    "                fields=fields,\n",
    "                filters=filters,\n",
    "                sort_object={\"submitter_id\": \"asc\"},\n",
    ")\n",
    "\n",
    "# display the returned data\n",
    "if 'data' in response:\n",
    "    case_data = response['data'][node]\n",
    "    display(case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elastic search is handy for accessing files for a cohort since object_ids associated with each study or case are joined to the table \n",
    "study_object_ids = []\n",
    "for study in study_data:\n",
    "    if 'object_id' in study:\n",
    "        object_id_list = study['object_id']\n",
    "        for object_id in object_id_list:\n",
    "            study_object_ids.append(object_id)\n",
    "display(study_object_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a0d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842abd3",
   "metadata": {},
   "source": [
    "### Sending aggregations with guppy\n",
    "---\n",
    "* Guppy has the ability to return some useful statistics (e.g., histograms) using aggregations.\n",
    "* The `Gen3Query.graphql_query()` function can be used to send aggregations and other more complex queries that the basic `Gen3Query.query()` function can't support: https://github.com/uc-cdis/gen3sdk-python/blob/8196cf4b76a65d0b9b31c8637a18dfac2a911b56/gen3/query.py#L112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27035fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A more complex example using Python requests\n",
    "query_txt = \"\"\"{\n",
    "    _aggregation {\n",
    "        case {\n",
    "          sex {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "          race {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "          ethnicity {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "response = query.graphql_query(query_string=query_txt)\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example simple script for sending a basic aggregation request that will return the data as a DataFrame (\"TSV\")\n",
    "## https://github.com/cgmeyer/gen3sdk-python/blob/5fd6b868374f622221c0c0173a0d9489b190facd/expansion/expansion.py#L3511\n",
    "\n",
    "data = exp.guppy_aggregation(node='case', prop='race', format='TSV')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcdb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A more complex example using Python requests\n",
    "query_txt = \"\"\"{\n",
    "    _aggregation {\n",
    "        case {\n",
    "          sex {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "          race {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "          ethnicity {\n",
    "            histogram {\n",
    "              key\n",
    "              count\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    }\n",
    "}\"\"\"\n",
    "query_json = {\"query\": query_txt}\n",
    "guppy_url = \"{}/guppy/graphql\".format(api)\n",
    "response = requests.post(guppy_url, json=query_json, auth=auth)\n",
    "display(json.loads(response.text)['data']['_aggregation']['case'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b30f1eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Count the number of files in each project\n",
    "files_by_project = \"\"\"\n",
    "{\n",
    "  _aggregation {\n",
    "    data_file {\n",
    "      project_id {\n",
    "        histogram {\n",
    "          key\n",
    "          count\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "response = query.graphql_query(files_by_project)\n",
    "display(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597b07f",
   "metadata": {},
   "source": [
    "### Use the guppy download endpoint to access ElasticSearch tables.\n",
    "---\n",
    "* Tables of data from ES can be exported from the data exploration app (https://data.midrc.org/explore) by using the \"Download Table\" button.\n",
    "* To get these sorts of tables using the API, you can use the guppy download function: https://github.com/uc-cdis/gen3sdk-python/blob/8196cf4b76a65d0b9b31c8637a18dfac2a911b56/gen3/query.py#L146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aae2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query gets all the imaging studies of modality \"CT\"\n",
    "\n",
    "query.raw_data_download(\n",
    "                    data_type=\"imaging_study\",\n",
    "                    fields=[\n",
    "                        \"study_uid\",\n",
    "                        \"project_id\",\n",
    "                        \"study_description\",\n",
    "                        \"body_part_examined\",\n",
    "                        \"case_ids\",\n",
    "                        \"object_id\"\n",
    "                    ],\n",
    "                    filter_object={\"=\": {\"study_modality\": \"CT\"}}\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279428e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example getting all the cases in a particular project between ages of 45 and 47\n",
    "\n",
    "query.raw_data_download(\n",
    "                    data_type=\"case\",\n",
    "                    fields=[\n",
    "                        \"submitter_id\",\n",
    "                        \"project_id\",\n",
    "                        \"race\",\n",
    "                        \"sex\",\n",
    "                        \"ethnicity\",\n",
    "                        \"age_at_index\",\n",
    "                        \"object_id\"\n",
    "                    ],\n",
    "                    filter_object={\"AND\": [{\">=\": {\"age_at_index\": 45}},\n",
    "                                           {\"<=\": {\"age_at_index\": 47}},\n",
    "                                           {\"=\": {\"project_id\": \"Open-A1\"}}]},\n",
    "                    sort_fields=[{\"submitter_id\": \"asc\"}],\n",
    "                    accessibility=\"accessible\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6570ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example getting all the imaging studies where the patient had a positive COVID-19 test result within a week of the study date.\n",
    "\n",
    "response = query.raw_data_download(\n",
    "                    data_type=\"imaging_study\",\n",
    "                    fields=[\n",
    "                        \"study_uid\",\n",
    "                        \"study_modality\",\n",
    "                        \"case_ids\",\n",
    "                        \"project_id\",\n",
    "                        \"race\",\n",
    "                        \"sex\",\n",
    "                        \"ethnicity\",\n",
    "                        \"age_at_index\",\n",
    "                        \"object_id\"\n",
    "                    ],\n",
    "                    filter_object={\"AND\": [{\">=\": {\"days_from_study_to_pos_covid_test\": -7}},\n",
    "                                           {\"<=\": {\"days_from_study_to_pos_covid_test\": 7}}]},\n",
    "                    sort_fields=[{\"submitter_id\": \"asc\"}],\n",
    "                    accessibility=\"accessible\"\n",
    "                )\n",
    "\n",
    "display(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc104d89",
   "metadata": {},
   "source": [
    "## Use the Gen3 SDK \"drs-pull\" commands to access the files themselves.\n",
    "---\n",
    "\n",
    "Next, we'll use the Gen3 SDK command `gen3 drs-pull object` to access the imaging file using it's \"object_id\" aka \"data GUID\".\n",
    "\n",
    "See the detailed documentation to learn more about the Gen3 SDK drs-pull command: https://github.com/uc-cdis/gen3sdk-python/blob/master/docs/howto/drsDownloading.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a look at one of the file objects\n",
    "\n",
    "data = response[0]\n",
    "case_id = data['case_ids'][0]\n",
    "study_uid = data['study_uid']\n",
    "object_id = data['object_id'][0]\n",
    "print(case_id)\n",
    "print(object_id)\n",
    "\n",
    "cmd = \"gen3 --auth {} --endpoint data.midrc.org drs-pull object {}\".format(cred,object_id)\n",
    "os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d6c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"ls -l {}/{}\".format(case_id,study_uid)\n",
    "stout = subprocess.check_output(cmd, shell=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the filename and series UID of the downloaded file using RegEx\n",
    "import re\n",
    "\n",
    "m = re.search(' ([0-9\\.]+.zip)', str(stout))\n",
    "\n",
    "if m:\n",
    "    zip_file = m.group(1)\n",
    "    print(zip_file)\n",
    "else:\n",
    "    print(\"No zip found.\")\n",
    "\n",
    "series_uid = re.sub(\"(\\.zip)\", \"\", zip_file)\n",
    "print(series_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the imaging series package\n",
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('{}/{}/{}/{}'.format(demo_dir,case_id,study_uid,zip_file), 'r') as zipObj:\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08956a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the name of the newly create .dcm file\n",
    "cmd = \"ls -l {}/{}/{}\".format(case_id,study_uid,series_uid)\n",
    "stout = subprocess.run(cmd, shell=True, capture_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the name of the first DICOM file in the extracted imaging series\n",
    "m = re.search(' ([0-9\\.]+.dcm)', str(stout))\n",
    "\n",
    "if m:\n",
    "    dcm_file = m.group(1)\n",
    "    print(dcm_file)\n",
    "else:\n",
    "    print(\"No DCM files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90575758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the DCM file using the python DICOM package pydicom\n",
    "dimg = pydicom.dcmread(\"{}/{}/{}/{}\".format(case_id,study_uid,series_uid,dcm_file),force=True)\n",
    "dimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd85f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
