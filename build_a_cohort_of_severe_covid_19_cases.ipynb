{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ef63c3",
   "metadata": {
    "id": "d8ef63c3"
   },
   "source": [
    "# How to Build a Cohort of Severe COVID-19 Cases Using the MIDRC Data Commons\n",
    "---\n",
    "This notebook demonstrates how to build a cohort of severe COVID-19 cases using patient clinical data and AI research-based annotations in the MIDRC data commons.\n",
    "\n",
    "Our goal is to download structured data and files for 2 related cohorts: 1) severe COVID cases and 2) a control cohort of non-severe COVID cases.\n",
    "\n",
    "Luckily for the patients, there are many more non-severe cases; but that presents a challenge for building a balanced dataset that is optimal for AI/ML training and evaluation.\n",
    "\n",
    "* Cohort 1: All chest x-rays (CXR) with an mRALE score of 10 or higher obtained within 2 days after a positive COVID test.\n",
    "\n",
    "* Cohort 2: Matching number of CXRs with an mRALE score <10 obtained within 2 days after a positive COVID test.\n",
    "\n",
    "* Additionally, we want the cohorts to be somewhat balanced and matched in terms of the demographics: age, sex, race, and ethnicity.\n",
    "\n",
    "\n",
    "by Chris Meyer, PhD\n",
    "\n",
    "Manager of Data and User Services at the Center for Translational Data Science at University of Chicago\n",
    "\n",
    "August 2023\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7f87c",
   "metadata": {
    "id": "5db7f87c"
   },
   "source": [
    "## 1) Set up Python environment\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bffd5d4",
   "metadata": {
    "id": "1bffd5d4"
   },
   "source": [
    "### Set local variables\n",
    "---\n",
    "Change the following directory paths to a valid working directories where you're running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c59c2c",
   "metadata": {
    "id": "c3c59c2c"
   },
   "outputs": [],
   "source": [
    "cred = \"/content/credentials.json\" # location of your MIDRC credentials, downloaded from https://data.midrc.org/identity by clicking \"Create API key\" button and saving the credentials.json locally; then upload to Colab Files browser\n",
    "api = \"https://data.midrc.org\" # The base URL of the data commons being queried. This shouldn't change for MIDRC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962e54c",
   "metadata": {
    "id": "7962e54c"
   },
   "source": [
    "### Install / Import Python Packages and Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a7935",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e1a7935",
    "outputId": "d6a996a2-3321-4f99-e2f0-594a0b7a9f9a"
   },
   "outputs": [],
   "source": [
    "## The packages below may be necessary for users to install according to the imports necessary in the subsequent cells.\n",
    "\n",
    "import sys\n",
    "#!{sys.executable} -m pip install\n",
    "#!{sys.executable} -m pip install --upgrade pandas\n",
    "#!{sys.executable} -m pip install --upgrade --ignore-installed PyYAML\n",
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade gen3\n",
    "#!{sys.executable} -m pip install pydicom\n",
    "#!{sys.executable} -m pip install IPython",
    "#!{sys.executable} -m pip install psmpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2fa09",
   "metadata": {
    "id": "7ea2fa09"
   },
   "outputs": [],
   "source": [
    "## Import Python Packages and scripts\n",
    "\n",
    "import os, subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import pydicom\n",
    "\n",
    "# import some Gen3 packages\n",
    "import gen3\n",
    "from gen3.auth import Gen3Auth\n",
    "from gen3.query import Gen3Query\n",
    "from IPython.display import display",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784ecc9",
   "metadata": {
    "id": "7784ecc9"
   },
   "source": [
    "### Initiate instances of the Gen3 SDK Classes using credentials file for authentication\n",
    "---\n",
    "Again, make sure the \"cred\" directory path variable reflects the location of your credentials file (path variables set above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316dcdf",
   "metadata": {
    "id": "d316dcdf"
   },
   "outputs": [],
   "source": [
    "auth = Gen3Auth(api, refresh_file=cred) # authentication class\n",
    "query = Gen3Query(auth) # query class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea96784-3370-46a6-a2c7-edda947bfa8f",
   "metadata": {
    "id": "2ea96784-3370-46a6-a2c7-edda947bfa8f"
   },
   "source": [
    "## 2) Build Cohorts by Sending Queries to the MIDRC APIs\n",
    "#### General notes on sending queries:\n",
    "* There are many ways to query and access metadata for cohort building in MIDRC, but this notebook will focus on using the [Gen3](https://gen3.org) graphQL query service [\"guppy\"](https://github.com/uc-cdis/guppy/#readme). This is the backend query service that [MIDRC's data explorer GUI](https://data.midrc.org/explorer) uses. So, anything you can do in the explorer GUI, you can do with guppy queries, and more!\n",
    "* The guppy graphQL service has more functionality than is demonstrated in this simple example with extensive documentation in GitHub [here](https://github.com/uc-cdis/guppy/blob/master/doc/queries.md) in case you'd like to build your own queries from scratch.\n",
    "* The Gen3 SDK (intialized as \"query\" above in this notebook) has Python wrapper scripts to make sending queries to the guppy graphQL API simpler. The guppy SDK package can be viewed in GitHub [here](https://github.com/uc-cdis/gen3sdk-python/blob/master/gen3/query.py).\n",
    "* Guppy queries focus on a particular type of data (cases, imaging studies, files, etc.) and include arguments that are akin to selecting filter values in [MIDRC's data explorer GUI](https://data.midrc.org/explorer).\n",
    "* To see more documentation about to use and combine filters with various operator logic (like AND/OR/IN, etc.) see [this page](https://github.com/uc-cdis/guppy/blob/master/doc/queries.md#filter).\n",
    "* We then send our query to MIDRC's guppy API endpoint using [the Gen3Query SDK package](https://github.com/uc-cdis/gen3sdk-python/blob/master/gen3/query.py) we initialized earlier.\n",
    "* If our query request is successful, the API response should be in JSON format, and it should contain a list of patient IDs along with any other patient data we ask for.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4a624e-f769-46a9-b76c-2efe9713bf61",
   "metadata": {
    "id": "3e4a624e-f769-46a9-b76c-2efe9713bf61"
   },
   "source": [
    "#### Cohort 1: All chest x-rays (CXR) with an mRALE score of 10 or higher obtained within 2 days after a positive COVID test.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4a36e-0647-4546-9777-6d0ad7b32750",
   "metadata": {
    "id": "9bc4a36e-0647-4546-9777-6d0ad7b32750"
   },
   "outputs": [],
   "source": [
    "### Set some \"imaging_study\" query parameters\n",
    "\n",
    "## mRALE filter: we'll select all imaging studies annotated with an mRAle scores greater than or equal to this threshold number\n",
    "mRALE_threshold = 10\n",
    "\n",
    "## days from study to positive COVID-19 test filter: we want imaging studies performed within two days after a positive test\n",
    "min_days_from_study_to_test = -2\n",
    "max_days_from_study_to_test = 0\n",
    "\n",
    "## Imaging study modality filter: we want chest x-rays, so we want studies with a modality of either DX or CR\n",
    "study_modalities = [\"DX\", \"CR\"]\n",
    "\n",
    "## Imaging study body part filter: here we select \"chest\" as the \"LOINC system\" filter, which is the body part examined\n",
    "body_part_examined = \"Chest\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910b3e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8910b3e4",
    "outputId": "5ecb442a-edc0-4e65-98c0-20b0f33d62a7"
   },
   "outputs": [],
   "source": [
    "## Note: the \"fields\" option defines what fields we want the query to return. If set to \"None\", returns all available fields.\n",
    "\n",
    "severe_studies = query.raw_data_download(\n",
    "                    data_type=\"imaging_study\",\n",
    "                    fields=None,\n",
    "                    filter_object={\n",
    "                        \"AND\": [\n",
    "                            {\"IN\": {\"loinc_system\": [body_part_examined]}},\n",
    "                            {\"IN\": {\"study_modality\": study_modalities}},\n",
    "                            {\"nested\": {\"path\": \"imaging_study_annotations\", \">=\": {\"midrc_mRALE_score\": mRALE_threshold}}},\n",
    "                            {\"AND\": [\n",
    "                                {\">=\": {\"days_from_study_to_pos_covid_test\": min_days_from_study_to_test}},\n",
    "                                {\"<=\": {\"days_from_study_to_pos_covid_test\": max_days_from_study_to_test}}\n",
    "                            ]}\n",
    "                        ]\n",
    "                    },\n",
    "                    sort_fields=[{\"submitter_id\": \"asc\"}]\n",
    "                )\n",
    "\n",
    "if len(severe_studies) > 0 and \"submitter_id\" in severe_studies[0]:\n",
    "    severe_study_ids = [i['submitter_id'] for i in severe_studies] ## make a list of the imaging study IDs returned\n",
    "    print(\"Query returned {} study IDs.\".format(len(severe_studies)))\n",
    "    print(\"Data is a list with rows like this:\\n\\t {}\".format(severe_studies[0:1]))\n",
    "else:\n",
    "    print(\"Your query returned no data! Please, check that query parameters are valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df88d18-e709-44a8-a395-98a05126cff3",
   "metadata": {
    "id": "8df88d18-e709-44a8-a395-98a05126cff3"
   },
   "source": [
    "#### Cohort 2: CXRs with an mRALE score <10 obtained within 2 days after a positive COVID test.\n",
    "---\n",
    "\n",
    "We don't need to set any new parameters for our filters this time. We just need to reverse the operator on the mRALE threshold from greater than or equal to (`>=`) to less than (`<`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1ce8c-ccd3-468d-8df3-c435c5e83148",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eee1ce8c-ccd3-468d-8df3-c435c5e83148",
    "outputId": "fbfeb125-028b-4d64-895d-1157e8663302"
   },
   "outputs": [],
   "source": [
    "mild_studies = query.raw_data_download(\n",
    "                    data_type=\"imaging_study\",\n",
    "                    fields=None,\n",
    "                    filter_object={\n",
    "                        \"AND\": [\n",
    "                            {\"IN\": {\"loinc_system\": [body_part_examined]}},\n",
    "                            {\"IN\": {\"study_modality\": study_modalities}},\n",
    "                            {\"nested\": {\"path\": \"imaging_study_annotations\", \"<\": {\"midrc_mRALE_score\": mRALE_threshold}}},\n",
    "                            {\"AND\": [\n",
    "                                {\">=\": {\"days_from_study_to_pos_covid_test\": min_days_from_study_to_test}},\n",
    "                                {\"<=\": {\"days_from_study_to_pos_covid_test\": max_days_from_study_to_test}}\n",
    "                            ]}\n",
    "                        ]\n",
    "                    },\n",
    "                    sort_fields=[{\"submitter_id\": \"asc\"}]\n",
    "                )\n",
    "\n",
    "if len(mild_studies) > 0 and \"submitter_id\" in mild_studies[0]:\n",
    "    mild_study_ids = [i['submitter_id'] for i in mild_studies] ## make a list of the imaging study IDs returned\n",
    "    print(\"Query returned {} study IDs.\".format(len(mild_studies)))\n",
    "    print(\"Data is a list with rows like this:\\n\\t {}\".format(mild_studies[0:1]))\n",
    "else:\n",
    "    print(\"Your query returned no data! Please, check that query parameters are valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33093eff-621f-452b-a0af-89af1940c65f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "id": "33093eff-621f-452b-a0af-89af1940c65f",
    "outputId": "023fcc64-dba1-4d9a-c101-800897b4002c"
   },
   "outputs": [],
   "source": [
    "severe_df = pd.DataFrame(severe_studies)\n",
    "display(severe_df.head())\n",
    "\n",
    "mild_df = pd.DataFrame(mild_studies)\n",
    "display(mild_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f12059-e9c5-4dda-9844-8d88c8f3671c",
   "metadata": {
    "id": "a9f12059-e9c5-4dda-9844-8d88c8f3671c"
   },
   "outputs": [],
   "source": [
    "## Label cases as mild or severe and then combine the dataframes into a single dataframe\n",
    "mild_df['cohort'] = 'mild'\n",
    "severe_df['cohort'] = 'severe'\n",
    "df = pd.concat([mild_df,severe_df],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6eac91-594b-4532-8612-040518dc768d",
   "metadata": {
    "id": "9c6eac91-594b-4532-8612-040518dc768d"
   },
   "outputs": [],
   "source": [
    "# convert patient demographic columns in lists to strings\n",
    "df['case_ids'] = df['case_ids'].apply(lambda x: ','.join(map(str, x)))\n",
    "df['ethnicity'] = df['ethnicity'].apply(lambda x: ','.join(map(str, x)))\n",
    "df['race'] = df['race'].apply(lambda x: ','.join(map(str, x)))\n",
    "df['age_at_index'] = df['age_at_index'].apply(lambda x: ','.join(map(str, x)))\n",
    "df['age_at_index'] = df['age_at_index'].astype(int)\n",
    "df['sex'] = df['sex'].apply(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "# add binned ages for calculating age distributions later\n",
    "age_bins = np.arange(10,100,10)\n",
    "df['age_bin'] = pd.cut(df['age_at_index'], bins=age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a61074-2105-4b10-93de-f1baec4f31ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7a61074-2105-4b10-93de-f1baec4f31ee",
    "outputId": "e5eff9b4-d415-4a9d-f7cf-d1c310734f96"
   },
   "outputs": [],
   "source": [
    "# The dataset is inbalanced with more mild COVID patients than severe\n",
    "df['cohort'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a150187-a6d7-4e94-8f0e-e952ba73de09",
   "metadata": {
    "id": "6a150187-a6d7-4e94-8f0e-e952ba73de09"
   },
   "source": [
    "## 3) Now we use re-sampling techniques to balance the dataset\n",
    "---\n",
    "In order to create a mild COVID cohort of the same size as the smaller severe COVID cohort that roughly matches the demographics of the smaller cohort, we need to sample cases from the larger mild COVID cohort through a process called \"undersampling\" until the size of the two cohorts is equal.\n",
    "\n",
    "\"Undersampling\" refers to a group of techniques designed to balance the class distribution for a classification dataset that has a skewed class distribution and is a common technique used in machine learning to balance imbalanced datasets. In this case, we want to undersample the larger patient cohort while ensuring that the resulting two cohorts have a similar distribution of four demographic variables: sex, race, ethnicity, and age.\n",
    "\n",
    "The following column headers in the Pandas DataFrame we created above will be used for our sampling script: `sex`, `ethnicity`, `race`, and `age_at_index`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee79442-e1f3-4575-ad3e-10de028da958",
   "metadata": {
    "id": "0ee79442-e1f3-4575-ad3e-10de028da958"
   },
   "source": [
    "### Calculate the Size of the Smaller Cohort:\n",
    "\n",
    "Determine the size you want for the smaller cohort. If both cohorts need to be of the same size, you can calculate the size as the minimum size of the two cohorts. You can do this by using the `min` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7040419-ed23-4cee-ac72-b03abb18184f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "f7040419-ed23-4cee-ac72-b03abb18184f",
    "outputId": "254ce3ba-f5c8-4ca1-c958-65b247aa1962"
   },
   "outputs": [],
   "source": [
    "cohorts = ['mild', 'severe']\n",
    "cohort_sizes = {}\n",
    "for cohort in cohorts:\n",
    "    cohort_sizes[cohort] = len(df[df['cohort']==cohort])\n",
    "display(cohort_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7cd74-6583-4812-a0ad-e18c4b3e3559",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55b7cd74-6583-4812-a0ad-e18c4b3e3559",
    "outputId": "f45b24fa-481b-422d-a316-eebdaf78c783"
   },
   "outputs": [],
   "source": [
    "smaller_cohort_size = min(cohort_sizes.values())\n",
    "smaller_cohort_name = list(cohort_sizes.keys())[list(cohort_sizes.values()).index(smaller_cohort_size)]\n",
    "sdf = df.loc[df['cohort']==smaller_cohort_name] # smaller cohort DataFrame\n",
    "\n",
    "larger_cohort_size = max(cohort_sizes.values())\n",
    "larger_cohort_name = list(cohort_sizes.keys())[list(cohort_sizes.values()).index(larger_cohort_size)]\n",
    "ldf = df.loc[df['cohort']==larger_cohort_name] # larger cohort DataFrame\n",
    "\n",
    "print(\"The smaller cohort is '{}' with a size of '{}'.\".format(smaller_cohort_name,smaller_cohort_size))\n",
    "print(\"The larger cohort is '{}' with a size of '{}'.\".format(larger_cohort_name,larger_cohort_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee95c5-66ca-4897-89f9-c833a252d511",
   "metadata": {
    "id": "93ee95c5-66ca-4897-89f9-c833a252d511"
   },
   "source": [
    "### Undersampling:\n",
    "\n",
    "Now, we undersample the larger cohort to match the smaller cohort's size while maintaining the desired distribution of demographic variables. For this, we'll use the `sample` function in Pandas.\n",
    "\n",
    "To use the Pandas `sample` function to undersample the larger cohort (mild COVID cases) while considering the four demographic variables and their distribution in the smaller cohort (severe COVID cases), you can create a custom sampling probability based on the distribution of the smaller cohort.\n",
    "\n",
    "#### Strategy\n",
    "---\n",
    "1) Determine the frequency of all combinations of demographic properties in the smaller cohort,\n",
    "2) Add this frequency to each row in the larger cohort by matching the demographics combinations, and\n",
    "3) Undersample the larger cohort using the inverse of these frequencies as weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b77fb61-addd-4a99-8209-e9e6178158c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1b77fb61-addd-4a99-8209-e9e6178158c9",
    "outputId": "69e400ba-c119-440f-d126-39e61e962714"
   },
   "outputs": [],
   "source": [
    "# Make a list of all combinations of demographic variables found in the master DataFrame using pd.value_counts()\n",
    "\n",
    "# list of properties to consider (dprops: \"demographic properties\")\n",
    "#dprops = ['sex','ethnicity']\n",
    "dprops = ['sex','ethnicity','race','age_bin']\n",
    "\n",
    "print(\"Counts of Demographic Property Combinations in Master DataFrame:\")\n",
    "mvc = df[dprops].value_counts() # mvc: \"master value counts\"\n",
    "#mvc[('Male', 'Not Hispanic or Latino')] # this is how you access individual values\n",
    "display(mvc)\n",
    "\n",
    "print(\"\\nAll Combinations of Demographic Properties in Master DataFrame:\")\n",
    "combos = mvc.index.tolist()\n",
    "display(combos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6aa1e-2945-459c-aaa3-daeb455d086f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ec6aa1e-2945-459c-aaa3-daeb455d086f",
    "outputId": "06b5b12d-8b14-41d7-e4ea-7a23ac86e732"
   },
   "outputs": [],
   "source": [
    "# Now look at the frequencies of each demographic combo in the smaller cohort\n",
    "svc = sdf[dprops].value_counts(normalize=False).reindex(combos) # svc: \"smaller cohort value counts\"\n",
    "print(\"Smaller Cohort Demographics Value Counts\\n{}\\n\\n\".format(svc))\n",
    "\n",
    "# use normalize=True to get the relative frequencies (count of a demographic / sum of all demographics)\n",
    "svf = sdf[dprops].value_counts(normalize=True).reindex(combos) # svf: \"smaller cohort value frequencies\"\n",
    "print(\"Smaller Cohort Demographics Frequencies\\n{}\\n\\n\".format(svf))\n",
    "\n",
    "# sampling weights should be the inverse of their frequencies; less frequent demographics get a higher probability of sampling\n",
    "svw = 1/svf # svw: \"smaller cohort value weights\"\n",
    "print(\"Smaller Cohort Demographics Weights for Undersampling\\n{}\\n\\n\".format(svw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b2126-e0c2-44c6-9712-c74b5643f0d5",
   "metadata": {
    "id": "133b2126-e0c2-44c6-9712-c74b5643f0d5"
   },
   "source": [
    "Demographics in the larger cohort not represented in the smaller cohort get NaN for counts. Here we'll convert the `NaN`s to `0` to use as weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b737a4c-e4af-4c72-beba-63dddc650a83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "2b737a4c-e4af-4c72-beba-63dddc650a83",
    "outputId": "9355c255-ae6e-47a4-e166-86ff20d54faf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace NaNs with 0:\n",
    "for key in list(svw.keys()):\n",
    "    if np.isnan(svw[key]):\n",
    "        svw[key] = 0\n",
    "display(svw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7277895-c364-4512-917e-51490b1bb2d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7277895-c364-4512-917e-51490b1bb2d7",
    "outputId": "fc91006a-7f54-4938-ac3c-afe4107fb55f"
   },
   "outputs": [],
   "source": [
    "# Now apply the weights to each row of the larger cohort (mild COVID cases) to use in undersampling\n",
    "for combo in combos:\n",
    "    print(\"{}: {}\".format(combo,svw[combo]))\n",
    "    ldf.loc[(ldf[dprops[0]] == combo[0]) & (ldf[dprops[1]] == combo[1]) & (ldf[dprops[2]] == combo[2]) & (ldf[dprops[3]] == combo[3]),'weight'] = svw[combo]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24965406-a2ee-41e4-8e0c-33e231ff8e16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "id": "24965406-a2ee-41e4-8e0c-33e231ff8e16",
    "outputId": "6f423178-034d-4536-9467-6a5917dd49c0"
   },
   "outputs": [],
   "source": [
    "# Double check that all rows in the larger cohort were assigned a weight. If this is an empty DataFrame, then each row has a non-NaN weight.\n",
    "ldf.loc[ldf['weight'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9dad0-d020-471f-8e7f-4eec7aba59f7",
   "metadata": {
    "id": "17a9dad0-d020-471f-8e7f-4eec7aba59f7"
   },
   "source": [
    "### Undersample the Larger Cohort:\n",
    "\n",
    "Use the `sample` function with the calculated weights to undersample the larger cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf5f4c-304a-4b20-b297-9183de58483d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "id": "cfbf5f4c-304a-4b20-b297-9183de58483d",
    "outputId": "ed52b40e-172d-406e-a708-dbc025a5e9e3"
   },
   "outputs": [],
   "source": [
    "# Undersample the larger cohort (mild COVID cases) using weights\n",
    "\n",
    "udf = ldf.sample(n=smaller_cohort_size, weights=ldf['weight'], random_state=np.random.RandomState(41)) # undersampled larger cohort DataFrame, can set random_state in order to have reproducible sampling\n",
    "#udf = ldf.sample(n=smaller_cohort_size, weights=ldf['weight']) # undersampled larger cohort DataFrame, leave random_state out to get a non-reproducible, random sample\n",
    "udf.reset_index(drop=True,inplace=True)\n",
    "udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6eaa2-0f66-4a14-9284-63581785e084",
   "metadata": {
    "id": "9dc6eaa2-0f66-4a14-9284-63581785e084"
   },
   "source": [
    "### Combine the Two Cohorts:\n",
    "\n",
    "After undersampling, you'll have two cohorts of the same size, and the larger cohort should be balanced with respect to the four demographic variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093ca7c-79e2-478a-8c33-169c034b909a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "id": "7093ca7c-79e2-478a-8c33-169c034b909a",
    "outputId": "c7a26214-b4b1-46d2-956e-a491ab209f56"
   },
   "outputs": [],
   "source": [
    "# Combine the undersampled larger cohort with the smaller cohort\n",
    "bdf = pd.concat([udf.drop(columns=[\"weight\"]), sdf]).reset_index(drop=True) # balanced DataFrame\n",
    "bdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b208d-eea3-4ea5-8658-fb18319934c9",
   "metadata": {
    "id": "005b208d-eea3-4ea5-8658-fb18319934c9"
   },
   "source": [
    "## 4) Verification:\n",
    "\n",
    "Ensure that the demographic distributions of the two cohorts are now balanced for all four variables. You can use the `pandas` `groupby` and `value_counts` methods or other appropriate methods to check the distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41T5PIOgPPR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a41T5PIOgPPR",
    "outputId": "345dd571-d327-4eef-df00-34e8261a0351"
   },
   "outputs": [],
   "source": [
    "# Use pandas groupby and plot functions to view relative counts of different demographics in the balanced cohort\n",
    "for prop in dprops:\n",
    "  dfu = bdf.groupby([prop],observed=True).cohort.value_counts().unstack()\n",
    "  ax = dfu.plot(kind='bar', figsize=(7, 5), xlabel=prop, ylabel='Count', rot=90) # change rot=0 or rot=45 to change x-axis label display angle\n",
    "  ax.legend(title='cohort', bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544761e9",
   "metadata": {
    "id": "544761e9"
   },
   "source": [
    "## The End\n",
    "---\n",
    "If you have any questions related to this notebook don't hesitate to reach out to the MIDRC Helpdesk at midrc-support@datacommons.io or the author directly at cgmeyer@uchicago.edu\n",
    "\n",
    "Happy data wrangling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6691638",
   "metadata": {
    "id": "f6691638"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
